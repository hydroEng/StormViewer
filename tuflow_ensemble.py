import os
import statistics
import pandas
import pandas as pd
import re

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)


def get_po_csvs(input_dir: str) -> list:
    """
    This function detects TUFLOW PO csv outputs and saves their filepaths in a list.

    Args:
        input_dir (str): Path to folder containing PO output CSVs.

    Returns:
        list: List of absolute filepaths (str) of all detected CSVs

    """

    csv_filepaths = []

    for file in os.listdir(input_dir):
        if '_po.csv' in file.lower():
            csv_filepaths.append(os.path.join(input_dir, file))
    return csv_filepaths


def parse_po_csv(input_file: str) -> pd.DataFrame:
    """
    This function converts PO_line CSVs to pandas dataframes.

    Args:
        input_file: Path to PO file CSV.

    Returns:
        Pandas DataFrame with cleaned data from csv. The name of the DataFrame follows the name of the csv file.
    """

    df = pd.read_csv(input_file)

    # Drop first column with the filename
    first_column = df.columns[0]
    df.drop(first_column, axis=1, inplace=True)

    # Set index as time increment
    df.set_index(df.columns[0], inplace=True)

    # Run ID read by input filename prepared by TUFLOW.
    df.name = os.path.basename(input_file)

    return df


def _parse_run_id(run_id: str) -> tuple[str, str, str]:
    """
    This function reads the .csv filename and parses it into storm event, duration and temporal pattern respectively.

    Args:
        run_id: Name of csv file.

    Returns:
        a 3x1 tuple of strings with storm, duration and temporal pattern.

    """

    run_id_l = run_id.lower()

    storm = re.search(r"\d{2,4}[.]?\d?y", run_id_l).group()
    duration = re.search(r"\d{1,4}m", run_id_l).group()
    temp_patt = re.search(r"tp\d*", run_id_l).group()

    return storm, duration, temp_patt


def _get_po_lines(po_df: pd.DataFrame) -> list[str]:
    """
    Grabs names of columns containing max flow, i.e. the po lines. Reads dataframes files generated by parse_po_csv()

    Args:
        po_df:

    Returns:
        A list of column names containing the PO_line names
    """
    po_lines = []

    for column, values in po_df.items():
        if 'Flow' in column:
            po_lines.append(po_df[column][0])

    return po_lines


def _get_all_max_flows(po_sr: pd.Series) -> pd.Series:
    """
    This function processes a cleaned dataframe containing PO line data and returns a pd.Series object with the
    maximum flow in each PO line for that run.

    Args:
        po_sr: Pandas DataFrame containing cleaned csv data.

    Returns:
        A pandas series containing the maximum flows for all PO lines in the run. The series .name is equal to the
        po_df name.
    """
    po_lines = _get_po_lines(po_sr)

    po_lines_columns = [f"Max Flow {s}" for s in po_lines]

    columns = ['Run ID', 'Event', 'Duration', 'Temporal Pattern'] + po_lines_columns

    po_max_flows = []

    for column, values in po_sr.items():
        if 'Flow' in column:
            # Get max flow in pd Series, ignoring text (typically PO line title)
            po_max_flows.append((pandas.to_numeric(po_sr[column],
                                                   errors='coerce').max()))

    run_id = po_sr.name

    run_id_values = list(_parse_run_id(run_id))
    new_row = [run_id] + run_id_values + po_max_flows

    sr = pd.Series(new_row, index=columns)
    sr.name = run_id

    return sr


def concat_po_srs(max_flows_dfs: list[pd.DataFrame]) -> pd.DataFrame:
    """
    This function reads a list of pd.Series containing dataframes with max flows for each run configuration. Only input
    dataframes generated by parse_po_csv().

    Args:
        max_flows_dfs: A list containing all dataframes to concatenate (should be output from get_all_max_flows).

    Returns:
        A DataFrame containing all results mimicking legacy spreadsheet.

    """

    all_runs_df = pd.DataFrame()

    for df in max_flows_dfs:
        new_row = _get_all_max_flows(df)
        all_runs_df = pd.concat([all_runs_df, new_row], axis=1)
    return all_runs_df.T


def _split_po_dfs(df: pd.DataFrame) -> list[pd.DataFrame]:
    """
    This function takes all results dataframe with multiple PO lines and splits the dataframes according to each po line
    for better manipulation.

    Args:
        df: DataFrame containing data for multiple PO lines.

    Returns:
        a list containing a DataFrame for each po_line.

    """

    po_lines = []
    po_line_dfs = []

    for column, values in df.items():
        if 'Max Flow' in column:
            po_lines.append(column)

    for po_line in po_lines:
        split_df = df
        excluded_columns = [x for x in po_lines if x != po_line]
        po_line_dfs.append(split_df.drop(columns=excluded_columns))

    return po_line_dfs


def _split_event(df: pd.DataFrame) -> list[pd.DataFrame]:
    """
    This function takes a dataframe with multiple events and splits the dataframes according to each event for better
    manipulation.

    Args:
        df: DataFrame containing data for multiple events

    Returns:
        a list containing a DataFrame for each po_line
    """

    unique_events = df['Event'].unique().tolist()

    event_dfs = []

    for event in unique_events:
        event_dfs.append(df[df['Event'] == event])

    return event_dfs


def _drop_sort_duration(df: pd.DataFrame) -> pd.DataFrame:
    sorted_df = df
    """
    This function drops all non-numeric chars from the index ('Duration') column in dataframe and sorts the dataframe by 
    numeric value.

    Args:
        df: DataFrame with 'Duration' column.

    Returns:
        a DataFrame sorted by duration (numeric value only).

    """

    # Remove alphabetical chars from minutes
    sorted_df.index = sorted_df.index.map(lambda x: (re.sub(r"[a-zA-Z]", "", str(x))))
    sorted_df.index = sorted_df.index.map(lambda x: int(x))
    sorted_df = sorted_df.sort_index()
    return sorted_df


def _get_col_name(value, input_row):
    """ Get column name of value in row generated by df.apply() method."""

    col_name = input_row[input_row == value].index[0]
    return col_name


def _get_crit_tp(row: pd.Series) -> str:
    """ This function is to be applied to processed dataframe of storm durations vs temporal patterns. Used
    to create a new column with critical storms for each duration. Do not use by itself, used via dataframe apply()
    method!

    Args:
        row: A pd.Series passed through via df.apply().

    Returns:
        a string containing name of column with critical storm.

    """
    median = row['Median']

    diffs = {}
    for cell in row:
        col_name = _get_col_name(cell, row)
        if 'tp' in col_name:
            diffs[col_name] = (cell - median)

    positive_diffs = {k: v for k, v in diffs.items() if v > 0}
    crit_tp = min(positive_diffs, key=positive_diffs.get)

    return crit_tp


def _tp_vs_max_flow_df(df: pd.DataFrame) -> tuple:
    """
    This function takes a df filtered by one event and one po_line and generates a dataframe presenting storm duration
    (x) vs temporal patterns (y), as well as average, median and critical temporal patterns for the run. Event and
    PO_line are stored in the DataFrame name.

    Note that po_line name will be lost in this process!

    Args:
        df: DataFrame with data for one event and po_line

    Returns:
        A DataFrame sorted by duration (x) vs temporal pattern (y), avg/median values, and critical storm.
    """
    event = df['Event'].unique().tolist()[0]

    po_line: str = ""

    for column, values in df.items():
        if 'Max Flow' in column:
            po_line = str(column)

    dur_tp_df = df.pivot(index='Duration', columns='Temporal Pattern', values=po_line)
    dur_tp_df.name = f"{po_line}: {event} Event"

    dur_tp_df = _drop_sort_duration(dur_tp_df)

    tp_cols = [col for col in dur_tp_df.columns if 'tp' in col]

    dur_tp_df['Average'] = dur_tp_df[tp_cols].mean(axis=1)
    dur_tp_df['Median'] = dur_tp_df[tp_cols].median(axis=1)

    dur_tp_df['Critical TP'] = dur_tp_df.apply(_get_crit_tp, axis=1)

    return event, po_line, dur_tp_df


def all_critical_storms(all_runs_df: pd.DataFrame) -> list[pd.DataFrame]:
    """
    This function outputs one dataframe for each event and PO line, representing all the temporal patterns and
    durations, as well as the critical storms for each duration.


    Args:
        all_runs_df: Pandas DataFrame with maximum flows provided against temporal pattern and duration.

    Returns:
        A list of dataframes each with max flow data + average, median and critical storms.

    """
    df = all_runs_df

    events = all_runs_df['Event'].unique()
    durations = all_runs_df['Duration'].unique()
    temp_patts = all_runs_df['Temporal Pattern'].unique()

    po_lines_dfs = _split_po_dfs(all_runs_df)

    working_dfs = []

    for _ in po_lines_dfs:
        final_dfs = _split_event(_)
        working_dfs.append(final_dfs)

    all_crit_tp_dfs = []
    for x in working_dfs:
        for y in x:
            event, po_line, df = _tp_vs_max_flow_df(y)
            sorted_df = _drop_sort_duration(df)
            sorted_df.name = f"{event}: {po_line}"
            all_crit_tp_dfs.append(sorted_df)

    return all_crit_tp_dfs


def summarize_results(crit_tp_df: pd.DataFrame):
    """
    This function reads a dataframe listing all critical storms and finds the duration / tp combination with highest
    critical storm.

    Args:
        crit_tp_df: Pandas DataFrame with critical storms and meta-statistics.

    Returns:
        Pandas Series summarizing critical storm configuration for a po_line.

    """
    max_med = crit_tp_df['Median'].max()
    crit_duration = crit_tp_df['Median'].idxmax()

    crit_tp = crit_tp_df.loc[crit_duration, 'Critical TP']

    event, po_line = str(crit_tp_df.name).split(':', 1)
    po_line = po_line.replace('Max Flow ', "")

    crit_max_flow = crit_tp_df.loc[crit_duration, crit_tp]

    index = ['Event', 'PO Line', 'Critical Duration', 'Critical TP', 'Critical TP Flow']
    values = [event, po_line, crit_duration, crit_tp, crit_max_flow]

    return pd.Series(data=values, index=index)


def main(input_path: str):
    raw_inputs = get_po_csvs(input_dir)
    all_max_flows = []

    for csv in raw_inputs:
        df1 = parse_po_csv(csv)
        all_max_flows.append(df1)

    df1 = concat_po_srs(all_max_flows)
    all_crit = all_critical_storms(df1)

    results_sr = []
    for df in all_crit:
        results_sr.append(summarize_results(df))

    results_df = pd.DataFrame(results_sr)
    print(results_df)


if __name__ == '__main__':
    input_dir = r"/home/Taha/tuflow_ensemble/C01/"
    all_runs = main(input_dir)
